# AI Text Detector

[![author](https://img.shields.io/badge/author-joaomr7-red.svg)](https://github.com/joaomr7)
[![](https://img.shields.io/badge/python-blue.svg)](https://www.python.org/downloads/release/python-365/)

This project is for my personal Data Science portfolio. The main idea is to develop an ML model to detect if a text was generated by a human or AI, and also deploy it on Streamlit Cloud.

<p align="center">
  <img src="images/AITextWarning.jpg">
</p>

<br>

# Getting Started

To download and build this project on your machine, use the following commands in your terminal:

1. `git clone --recursive https://github.com/joaomr7/ai_text_detector.git`

2. `pip install -r requirements.txt`

Obs: It is recommended to create an environment (with conda, for example) to build this project on your machine.

# Project Overview

With the arrival of LLMs (Large Language Models), one of the most common situations in today's world is people using AI systems, like ChatGPT, to create text content for them. This content can be used for doing homework, writing an article, creating a portfolio description, generating names, creating histories, simulate conversations, and thousands of other things.

The texts generated by these mechanisms are pretty well written and can convince anyone that the text wasn't written by a machine. That being said, the objective of this project is to develop a solution that can be able to distinguish whether a text was written by a human or machine.

This project follows the CRISP-DM methodology. To understand the correct organization of the entire project see the Jupyter notebook `1. Business Understanding` located at `notebooks/1. Business Understanding.ipynb`.

# About The Data

The dataset used in this project was obtained from Kaggle at this link: https://www.kaggle.com/datasets/sunilthite/llm-detect-ai-generated-text-dataset

The data consists of texts generated by humans and AIs.

# Main Insights

In the data Analysis part of this project I come with some interesting insights:

* Removing nouns from the dataset make the final model less dependent on the context of the training data;

* When I check the lexical diversity of the texts I conclude that some AIs texts are very similar to humans texts in terms of lexical diversity. But in some cases, AIs texts have more diversity;

* When looking for reading ease, humans texts appears to be better than AI texts (at least at the data I have).


# Data Preprocessing

To preprocess the data I used some libraries like Spacy, SymSpell and textstat. I used Spacy to tokenize the texts and generate the word2vec representation of the text. With SymSpell I fix the typos on the text. And with textstat I calculated the reading ease score of the text. In general the tranformation on the data follows these steps:

1. **text tokenization**

2. **stopwords removing**

3. **irrelevant information removing**

4. **nouns removing**

5. **typos fixing**

6. **text to vector**

`obs: irrelevant information can be that has no letters`


# Model

The best model for this task was Logistic Regression Classifier, that has a good performance with well balanced metrics on training and test data. Also this model gives a good importance for almost all the features, in general in takes into account the reading ease score, lexical diversity score and some parts of the word2vec representation of the text to evaluate if it was generates by human or AI.

# Model Comprehension

For better comprehension of the model results, I created some confidence levels, which are:

* Human text with low confidence;

* Human text with high confidence;

* AI text with low confidence;

* AI text with high confidence.

The idea is that high confidence stands on ranges where the model precision is higher.

# Deployment

This tool is deploy with streamlit, a python framework for data science projects. The interface of the app is very simple and tends to be fast to predict the results. The only thing that takes time is to load the model and preprocessor from memory (that happens just when you open app).


# Limitations

Even the model being pretty good with training and test data, in my testings with texts get mainly from Wikipedia and ChatGPT, I have noticed that he is good for detecting real AI texts that was generated with simple prompts. One big problem is that human texts some times are detect as `AI generated`.

Also if you ask, for example, to ChatGPT generated a text that looks more like a human, the text is not detected as `AI generated`.


# Future improvements

* In the future is a good idea to get a dataset with more examples and diversal content.
