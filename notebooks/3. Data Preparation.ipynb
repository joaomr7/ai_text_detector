{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preparation**\n",
    "\n",
    "In this section I'll create methods to preprocess the training data available in `artifacts/train.csv` (at root, after running data_ingestion.py).\n",
    "\n",
    "Based on the `2. Exploratory Data Analysis`, I have to do the following transfomations in the data:\n",
    "\n",
    "1. **Tokenize text**\n",
    "\n",
    "2. **Remove irrelevant information (can be anything that is not a letter)**\n",
    "\n",
    "3. **Count typos**\n",
    "\n",
    "4. **Correct Typos**\n",
    "\n",
    "5. **Count english contractions**\n",
    "\n",
    "6. **Remove stopwords**\n",
    "\n",
    "7. **Remove nouns**\n",
    "\n",
    "8. **Texts to vectors**\n",
    "\n",
    "\n",
    "**Obs:**\n",
    "* Before any text manipulation, the text need to be tokenize. That's why has `1.`;\n",
    "\n",
    "* For better usage, typos must be corrected before usage. That's why has `4.`;\n",
    "\n",
    "* I need a way to tranform the texts into vectors. And the way I'll handle it is with word2vec. That's why has `8.`;\n",
    "\n",
    "* I'll be using spaCy library to make all this things, except for counting and correcting typos, wich I'll use SymSpell for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import importlib.resources\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up spacy and disabling unused pipeline components\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner', 'lemmatizer', 'textcat', 'custom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up SymSpell to typo count\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=1, prefix_length=7)\n",
    "\n",
    "with importlib.resources.open_text('symspellpy', 'frequency_dictionary_en_82_765.txt') as file:\n",
    "    dictionary_path = file.name\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../artifacts/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In this essay I will talk about if the use of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The importance of self-care should never be un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you think this \"face\" was created by aliens...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThe potential impact of the use of social me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Many people believe that self-esteem comes fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated\n",
       "0  In this essay I will talk about if the use of ...          0\n",
       "1  The importance of self-care should never be un...          1\n",
       "2  Do you think this \"face\" was created by aliens...          0\n",
       "3  \\nThe potential impact of the use of social me...          1\n",
       "4  Many people believe that self-esteem comes fro...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show first rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Transformation**\n",
    "\n",
    "I'll make all the required tranformation into an unique function even to obtain better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(data):\n",
    "    '''\n",
    "    Helper function to prepare data to the ML model. \n",
    "    This function applies:\n",
    "        * text tokenization\n",
    "        * irrelevant information removing\n",
    "        * typos fixing\n",
    "        * stopwords removing\n",
    "        * nouns removing\n",
    "        * word2vec with spacy\n",
    "        \n",
    "    Also, this function count:\n",
    "        * contractions\n",
    "        * typos\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "    * data: text data to prepare the data\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    * a numpy array with the columns: texts tranformed into vector, contractions count, typos count\n",
    "\n",
    "    '''\n",
    "\n",
    "    # tranfrom texts in spacy docs (wich applies tokenization)\n",
    "    docs = [doc for doc in nlp.pipe(data)]\n",
    "\n",
    "    text_vectors = []\n",
    "    contractions_counts = []\n",
    "    typos_counts = []\n",
    "\n",
    "    contractions_patterns = [\n",
    "        r'\\b(\\w+)\\'(\\w+)\\b',\n",
    "        r'\\'(\\w+)\\b'\n",
    "    ]\n",
    "\n",
    "    for doc in docs:\n",
    "\n",
    "        # count contractions\n",
    "        contractions_count = 0\n",
    "        for pattern in contractions_patterns:\n",
    "            matches = re.findall(pattern, doc.text)\n",
    "            contractions_count += len(matches)\n",
    "\n",
    "        contractions_counts.append(contractions_count)\n",
    "\n",
    "        # count typos\n",
    "        typos_count = 0\n",
    "\n",
    "        words = []\n",
    "        spaces = []\n",
    "        for token in doc:\n",
    "            if not re.match(r'[a-zA-Z]|\\'[a-zA-Z]', token.text): # ignore irrelevant information\n",
    "                continue\n",
    "\n",
    "            if token.is_stop: # ignore stopwords\n",
    "                continue\n",
    "\n",
    "            # lookup for typo\n",
    "            correct_word = ''\n",
    "            suggestions = sym_spell.lookup(token.text, Verbosity.CLOSEST, max_edit_distance=1) # lookup for typo\n",
    "\n",
    "            if not suggestions:\n",
    "                correct_word = token.text\n",
    "                typos_count += 1\n",
    "\n",
    "            elif suggestions[0].term != token.text:\n",
    "                correct_word = suggestions[0].term\n",
    "                typos_count += 1\n",
    "\n",
    "            else:\n",
    "                correct_word = token.text\n",
    "\n",
    "            if token.pos_ in ['NOUN', 'PROPN']: # ignore nouns\n",
    "                continue\n",
    "\n",
    "            words.append(correct_word)\n",
    "            spaces.append(token.whitespace_)\n",
    "\n",
    "        typos_counts.append(typos_count)\n",
    "\n",
    "        clean_doc = Doc(vocab=nlp.vocab, words=words, spaces=spaces)\n",
    "        text_vectors.append(clean_doc.vector)\n",
    "\n",
    "    # return numpy array\n",
    "    return np.column_stack([text_vectors, contractions_counts, typos_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "prepared_data = prepare_model_data(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data shape: (23316, 302)\n"
     ]
    }
   ],
   "source": [
    "print('Prepared data shape:', prepared_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing to do with the data is to apply scaling. For simplicity I'll apply normalization in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "prepared_data_scaled = scaler.fit_transform(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35434973, 0.50383901, 0.33725433, ..., 0.3965236 , 0.01666667,\n",
       "        0.01581722],\n",
       "       [0.35296487, 0.58696288, 0.25450668, ..., 0.54237329, 0.01666667,\n",
       "        0.01054482],\n",
       "       [0.23955984, 0.47497949, 0.38730878, ..., 0.32879322, 0.06666667,\n",
       "        0.03690685],\n",
       "       ...,\n",
       "       [0.35158229, 0.66196401, 0.28397271, ..., 0.48455537, 0.        ,\n",
       "        0.00527241],\n",
       "       [0.48947814, 0.65405018, 0.21185979, ..., 0.44857336, 0.06666667,\n",
       "        0.03690685],\n",
       "       [0.33425374, 0.5884441 , 0.29810287, ..., 0.32490683, 0.08333333,\n",
       "        0.13884007]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show result\n",
    "prepared_data_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
